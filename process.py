# =====================================
# 1ï¸âƒ£ Import thÆ° viá»‡n
# =====================================
import os
import glob
import re
import pandas as pd
import nltk
import spacy
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# =====================================
# 2ï¸âƒ£ Äá»c vÃ  gá»™p dá»¯ liá»‡u
# =====================================
data_dir = r"D:\QuÃ¢n\project\movie_query\MovieData"
all_files = glob.glob(os.path.join(data_dir, "movies_out_*.csv"))

df_list = [pd.read_csv(f) for f in all_files]
combined_df = pd.concat(df_list, ignore_index=True)

print(f"âœ… ÄÃ£ Ä‘á»c {len(all_files)} file CSV, tá»•ng {len(combined_df)} dÃ²ng.")

# =====================================
# 3ï¸âƒ£ Chuáº©n bá»‹ NLP
# =====================================
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)

try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    from spacy.cli import download
    download("en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

# =====================================
# 4ï¸âƒ£ HÃ m lÃ m sáº¡ch vÄƒn báº£n
# =====================================
def clean_text_spacy(text):
    if pd.isna(text):
        return ""
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text.lower())
    doc = nlp(text)
    stop_words = set(stopwords.words('english'))
    tokens = [token.lemma_ for token in doc if token.text not in stop_words and token.text.strip()]
    return " ".join(tokens)

# =====================================
# 5ï¸âƒ£ LÃ m sáº¡ch cÃ¡c trÆ°á»ng quan trá»ng
# =====================================
combined_df["clean_title"] = combined_df["title"].apply(clean_text_spacy)
combined_df["clean_plot"] = combined_df["plot"].apply(clean_text_spacy)
if "genre" in combined_df.columns:
    combined_df["clean_genres"] = combined_df["genre"].apply(clean_text_spacy)
else:
    combined_df["clean_genres"] = ""

# Náº¿u cÃ³ poster_url thÃ¬ rename Ä‘á»ƒ tiá»‡n dÃ¹ng
if "poster" not in combined_df.columns and "poster_url" in combined_df.columns:
    combined_df["poster"] = combined_df["poster_url"]

# =====================================
# 6ï¸âƒ£ Táº¡o vÄƒn báº£n káº¿t há»£p cÃ³ trá»ng sá»‘
# =====================================
title_weight = 3.0
genre_weight = 2.0
plot_weight = 1.0

def combine_weighted_text(row):
    title_part = (row["clean_title"] + " ") * int(title_weight)
    genre_part = (row["clean_genres"] + " ") * int(genre_weight)
    plot_part = (row["clean_plot"] + " ") * int(plot_weight)
    return title_part + genre_part + plot_part

combined_df["weighted_text"] = combined_df.apply(combine_weighted_text, axis=1)

print("âœ… Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho TF-IDF")

# =====================================
# 7ï¸âƒ£ TF-IDF vÃ  tÃ¬m kiáº¿m
# =====================================
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(combined_df["weighted_text"])

print(f"âœ… TF-IDF matrix cÃ³ kÃ­ch thÆ°á»›c: {tfidf_matrix.shape}")

def search_tfidf_weighted(query, min_score=0.0):
    """
    ğŸ” TÃ¬m kiáº¿m phim dá»±a trÃªn TF-IDF cÃ³ trá»ng sá»‘ cho cÃ¡c trÆ°á»ng (title, genres, plot).
    - KhÃ´ng giá»›i háº¡n sá»‘ lÆ°á»£ng káº¿t quáº£.
    - Tráº£ vá» toÃ n bá»™ thÃ´ng tin cá»§a phim (ká»ƒ cáº£ poster).
    - Lá»c theo ngÆ°á»¡ng similarity (min_score).
    """
    query_clean = clean_text_spacy(query)
    query_vec = vectorizer.transform([query_clean])
    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()

    # Lá»c vÃ  sáº¯p xáº¿p theo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giáº£m dáº§n
    sorted_indices = similarities.argsort()[::-1]
    results = combined_df.iloc[sorted_indices].copy()
    results["similarity_score"] = similarities[sorted_indices]

    # Lá»c bá» phim cÃ³ Ä‘iá»ƒm quÃ¡ tháº¥p (náº¿u muá»‘n)
    results = results[results["similarity_score"] >= min_score]

    return results