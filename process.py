# =====================================
# process.py â€” Movie Search Engine Logic (tá»‘i Æ°u + cache)
# =====================================

import os
import glob
import re
import pandas as pd
import nltk
import spacy
import sqlite3
import pickle
import numpy as np
from functools import lru_cache
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel

# =====================================
# ÄÆ°á»ng dáº«n file database & model TF-IDF
# =====================================
DB_PATH = r"checkpoints/movies.db"
VEC_PATH = r"checkpoints/vectorizer.pkl"
MATRIX_PATH = r"checkpoints/tfidf_matrix.pkl"

# =====================================
# Chuáº©n bá»‹ NLP
# =====================================
try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords", quiet=True)

try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    from spacy.cli import download
    download("en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

# =====================================
# HÃ m lÃ m sáº¡ch vÄƒn báº£n
# =====================================
def clean_text_spacy(text):
    if pd.isna(text):
        return ""
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text.lower())
    doc = nlp(text)
    stop_words = set(stopwords.words("english"))
    tokens = [token.lemma_ for token in doc if token.text not in stop_words and token.text.strip()]
    return " ".join(tokens)

# =====================================
# Load database + TF-IDF model (náº¿u cÃ³)
# =====================================
if os.path.exists(DB_PATH) and os.path.exists(VEC_PATH) and os.path.exists(MATRIX_PATH):
    print("âœ… PhÃ¡t hiá»‡n file database & model â€” load nhanh!")

    conn = sqlite3.connect(DB_PATH)
    combined_df = pd.read_sql_query("SELECT * FROM movies", conn)
    conn.close()

    with open(VEC_PATH, "rb") as f:
        vectorizer = pickle.load(f)
    with open(MATRIX_PATH, "rb") as f:
        tfidf_matrix = pickle.load(f)

else:
    print("âš™ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u cÅ© â€” khá»Ÿi táº¡o tá»« CSV...")

    data_dir = r"D:\QuÃ¢n\project\movie_query\MovieData"
    all_files = glob.glob(os.path.join(data_dir, "movies_out_*.csv"))
    df_list = [pd.read_csv(f) for f in all_files]
    combined_df = pd.concat(df_list, ignore_index=True)

    print(f"âœ… ÄÃ£ Ä‘á»c {len(all_files)} file CSV, tá»•ng {len(combined_df)} dÃ²ng.")

    combined_df["clean_title"] = combined_df["title"].apply(clean_text_spacy)
    combined_df["clean_plot"] = combined_df["plot"].apply(clean_text_spacy)
    if "genre" in combined_df.columns:
        combined_df["clean_genres"] = combined_df["genre"].apply(clean_text_spacy)
    else:
        combined_df["clean_genres"] = ""

    if "poster" not in combined_df.columns and "poster_url" in combined_df.columns:
        combined_df["poster"] = combined_df["poster_url"]

    title_weight, genre_weight, plot_weight = 3, 2, 1

    def combine_weighted_text(row):
        return (
            (row["clean_title"] + " ") * title_weight
            + (row["clean_genres"] + " ") * genre_weight
            + (row["clean_plot"] + " ") * plot_weight
        )

    combined_df["weighted_text"] = combined_df.apply(combine_weighted_text, axis=1)
    print("âœ… Chuáº©n bá»‹ dá»¯ liá»‡u TF-IDF...")

    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(combined_df["weighted_text"])
    print(f"âœ… TF-IDF matrix: {tfidf_matrix.shape}")

    conn = sqlite3.connect(DB_PATH)
    combined_df.to_sql("movies", conn, if_exists="replace", index=False)
    conn.close()

    with open(VEC_PATH, "wb") as f:
        pickle.dump(vectorizer, f)
    with open(MATRIX_PATH, "wb") as f:
        pickle.dump(tfidf_matrix, f)

    print("ðŸ’¾ LÆ°u database & TF-IDF model thÃ nh cÃ´ng!")

# =====================================
# Nháº­n dáº¡ng loáº¡i truy váº¥n
# =====================================
def detect_query_type(query, df):
    q = query.lower().strip()

    # 1ï¸âƒ£ NÄƒm
    if re.search(r"\b(19|20)\d{2}\b", q):
        return "year"

    # 2ï¸âƒ£ Thá»ƒ loáº¡i
    all_genres = set()
    if "genre" in df.columns:
        for g in df["genre"].dropna():
            for gg in re.split(r"[,/|]+", str(g).lower()):
                gg = gg.strip()
                if gg:
                    all_genres.add(gg)
    if q in all_genres:
        return "genre"

    # 3ï¸âƒ£ NgÆ°á»i (diá»…n viÃªn/Ä‘áº¡o diá»…n)
    for col in ["cast", "director"]:
        if col in df.columns:
            if any(q in str(x).lower() for x in df[col].dropna().head(3000)):
                return "person"

    # 4ï¸âƒ£ TÃªn phim
    if any(q == str(t).lower() for t in df["title"].dropna()):
        return "title"

    # 5ï¸âƒ£ Máº·c Ä‘á»‹nh
    return "content"

# =====================================
# Cache hÃ³a bÆ°á»›c TF-IDF Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
# =====================================
@lru_cache(maxsize=256)
def cached_vector_search(query_clean):
    """Tráº£ vá» chá»‰ sá»‘ & Ä‘iá»ƒm cosine_similarity cá»§a query Ä‘Ã£ Ä‘Æ°á»£c vector hÃ³a."""
    query_vec = vectorizer.transform([query_clean])
    cosine_sim = linear_kernel(query_vec, tfidf_matrix).flatten()
    return cosine_sim

# =====================================
# HÃ m tÃ¬m kiáº¿m thÃ´ng minh
# =====================================
def smart_search(query, df=combined_df, top_n=10, min_score=0.0):
    query_type = detect_query_type(query, df)
    query_clean = clean_text_spacy(query)
    print(f"ðŸ” Kiá»ƒu truy váº¥n phÃ¡t hiá»‡n: {query_type}")

    # 1ï¸âƒ£ Thá»ƒ loáº¡i
    if query_type == "genre" and "genre" in df.columns:
        filtered = df[df["genre"].apply(lambda g: query.lower() in str(g).lower())]
        if not filtered.empty:
            sort_cols = [c for c in ["vote_count", "rating", "popularity"] if c in filtered.columns]
            if sort_cols:
                filtered = filtered.sort_values(by=sort_cols, ascending=False)
            return filtered.head(top_n)

    # 2ï¸âƒ£ NÄƒm
    if query_type == "year" and "year" in df.columns:
        year_match = re.search(r"\b(19|20)\d{2}\b", query)
        if year_match:
            year = int(year_match.group())
            filtered = df[df["year"] == year]
            sort_cols = [c for c in ["rating", "vote_count", "popularity"] if c in filtered.columns]
            if sort_cols:
                filtered = filtered.sort_values(by=sort_cols, ascending=False)
            return filtered.head(top_n)

    # 3ï¸âƒ£ NgÆ°á»i
    if query_type == "person":
        mask = pd.Series(False, index=df.index)
        for col in ["cast", "director"]:
            if col in df.columns:
                mask |= df[col].apply(lambda c: query.lower() in str(c).lower())
        filtered = df[mask]
        sort_cols = [c for c in ["rating", "vote_count", "popularity"] if c in filtered.columns]
        if sort_cols:
            filtered = filtered.sort_values(by=sort_cols, ascending=False)
        return filtered.head(top_n)

    # 4ï¸âƒ£ TÃªn phim
    if query_type == "title":
        filtered = df[df["title"].apply(lambda t: query.lower() in str(t).lower())]
        if not filtered.empty:
            sort_cols = [c for c in ["rating", "vote_count"] if c in filtered.columns]
            if sort_cols:
                filtered = filtered.sort_values(by=sort_cols, ascending=False)
        return filtered.head(top_n)

    # 5ï¸âƒ£ Ná»™i dung â€” TF-IDF Similarity (cÃ³ cache)
    cosine_sim = cached_vector_search(query_clean)
    # Náº¿u top_n lá»›n hÆ¡n tá»•ng sá»‘ phim => láº¥y toÃ n bá»™
    if top_n >= len(cosine_sim):
        top_sorted = np.argsort(cosine_sim)[::-1]  # sáº¯p toÃ n bá»™
    else:
        top_indices = np.argpartition(cosine_sim, -top_n)[-top_n:]
        top_sorted = top_indices[np.argsort(cosine_sim[top_indices])[::-1]]

    results = df.iloc[top_sorted].copy()
    results["similarity_score"] = cosine_sim[top_sorted]

    if "vote_count" in results.columns:
        vc = results["vote_count"].fillna(0).astype(float)
        vc_norm = (vc - vc.min()) / (vc.max() - vc.min() + 1e-9)
        results["combined_score"] = 0.7 * results["similarity_score"] + 0.3 * vc_norm
        results = results.sort_values(by="combined_score", ascending=False)
    else:
        results = results.sort_values(by="similarity_score", ascending=False)

    return results.head(top_n)

print("âœ… Module smart_search() + cache Ä‘Ã£ sáºµn sÃ ng!")